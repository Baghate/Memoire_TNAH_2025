\textbf{Annotation} = Cette tâche consiste à étiqueter et à organiser les données, en disant ce qu'il s'y trouve, de manière à ce qu’elles puissent être efficacement utilisées par les modèles de Machine Learning.

\textbf{Apprentissage auto supervisé} = \textit{Self-supervised learning}, est une position intermédiaire entre apprentissage non supervisé et supervisé. Le modèle apprend à partir de données non annotées mais structurées, en exploitant les relations intrinsèques entre les données. On l'utilise par exemple pour la supervision de parties manquantes d'une image ou pour générer les légendes d'images.

\textbf{Apprentissage machine} = \textit{Machine learning}, est une sous-catégorie de l'IA qui permet aux machines d'apprendre à partir de données sans être explicitement programmées pour chaque tâche, c'est basé sur des algorithmes qui optimisent leurs performances grâce à l'expérience.

\textbf{Apprentissage non supervisé} = \textit{Unsupervised learning}, ne comprends pas d'annotation de données, l’algorithme utilise un jeu de données brutes et obtient un résultat en se fondant sur la détection de similarités entre certaines de ces données. C'est l'approche du \textit{clustering} ou rapprochement par similarité, comme celui utilisé dans l'outil Panoptic.

\textbf{Apprentissage par renforcement} = \textit{Reinforcement learning}, processus dans lequel un programme extérieur évalue positivement ou négativement les résultats successifs de l’algorithme, l’accumulation des résultats permettant à l’algorithme d’améliorer ses performances jusqu’à ce qu’il atteigne un objectif préalablement fixé. L’apprentissage par renforcement est fréquemment utilisé dans la robotique. L’efficacité de l’apprentissage par renforcement a été attestée dans certains jeux stratégiques comme le jeu de go.

\textbf{Apprentissage profond} = \textit{Deep learning}, est une sous-catégorie de l'apprentissage machine qui utilise des réseaux de neurones artificiels profonds pour analyser des données complexes. Cette technique est capable de traiter des données volumineuses et non structurées (images, sons, vidéos), mais nécessite des ressources importantes en termes de puissances de calcul, ainsi qu'une grande quantité de données.

\textbf{Apprentissage supervisé} = \textit{Supervised learning}, repose sur l'utilisation de données annotées qui servent à former des algorithmes capables de classer des données ou de prédire des annotations. Ils nécessitent une intervention humaine. L’apprentissage supervisé recourt le plus souvent aux réseaux de neurones artificiels.

\textbf{Bash} = Interpréteur de commandes, sont équivalent Windows est le shell, il permet aux utilisateurs d'exécuter des instructions sur la ligne de commande, automatiser les tâches en utilisant des scripts et de gérer efficacement les fichiers, processus et ressources système.

\textbf{Boîte d'annotation} = \textit{Bounding box}, est un cadre rectangulaire tracé autour d'un objet ou d'une région d'intérêt dans une image numérique ou un espace 3D, afin d'en définir la position et la taille. Elle est couramment utilisée en vision par ordinateur, en traitement d'images et dans le développement de jeux vidéo pour des tâches telles que la détection d'objets, la détection de collisions ou l'annotation d'images. La boîte est généralement définie par les coordonnées de ses coins, par sa largeur et sa hauteur, ou par son centre et ses dimensions. Elle peut être alignée sur les axes (AABB) ou orientée (OBB) afin de mieux s'ajuster à la forme de l'objet. Bien qu'efficace et facile à calculer, une boîte englobante peut inclure des zones de fond et être moins précise que des méthodes plus détaillées comme la segmentation.

\textbf{Bruit} = Dans un processus automatisé, on parle de bruit quand les données contiennent des éléments indésirables, imprécis ou aléatoires qui ne reflètent pas fidèlement le signal ou l’information que l’on veut réellement analyser. Son opposé est le silence.

\textbf{CI/CD} = \textit{Continuous Integration/Continuous Delivery}, le CI/CD comble le fossé entre les activités et les équipes de développement et d'exploitation en imposant l'automatisation de la création, des tests et du déploiement des applications. Les pratiques DevOps usuelles peuvent impliquer le développement continu, le test continu, l'intégration continue, le déploiement continu et la surveillance continue des applications logicielles tout au long de leur cycle de vie.

\textbf{Clip} = \textit{Contrastive Language-Image Pre-training}, est un modèle de \textit{computer vision} développé par la société OpenAI (la société derrière ChatGPT), qui apprend à associer des images et leurs descriptions textuelles.


\textbf{CPU} = \textit{Central Processing Unit}, est constitué de milliards de transistors, le processeur (CPU) peut avoir plusieurs cœurs de traitement et est communément appelé le « cerveau » de l'ordinateur. Il est essentiel à tous les systèmes informatiques modernes, car il exécute les commandes et les processus nécessaires à votre ordinateur et à votre système d'exploitation.

\textbf{CSV} = \textit{Coma Separated Values}, est un fichier texte, par opposition aux formats dits « binaires ». Chaque ligne du texte correspond à une ligne du tableau et les virgules correspondent aux séparations entre les colonnes. Les portions de texte séparées par une virgule correspondent ainsi aux contenus des cellules du tableau.

\textbf{Centres de données} = \textit{Data centers}, sont des infrastructures composées d’un réseau d’ordinateurs et d’espaces de stockage. Ces infrastructures hautement sécurisées sont utilisées par les entreprises ou acteurs publics pour organiser, traiter, stocker et entreposer de grandes quantités de données.

\textbf{Data Scientist} = Généralement rattaché à la direction des systèmes d'information (DSI) d’une entreprise, le \textit{Data Scientist} a pour objectif d’analyser et d’exploiter toutes les datas des clients, des prospects ou bien encore des employés que l’entreprise récupère via différents canaux. L’objectif est de créer des modèles prédictifs et d’aider la prise de décision par la construction d'algorithmes.

\textbf{DevOps} = \textit{Development and operations}, est principalement par la promotion de l'automatisation et du suivi (monitoring) de toutes les étapes de la création d'un logiciel, depuis le développement, l'intégration, les tests, la livraison jusqu'au déploiement, l'exploitation et la maintenance des infrastructures.

\textbf{Docker} = Docker est une plateforme de conteneurisation open source qui permet de créer, déployer et gérer des applications virtualisées dans des conteneurs. Les conteneurs logiciels sont une forme de virtualisation légère qui offre une isolation pour les applications, rendant leur déploiement plus rapide et plus efficace par rapport aux machines virtuelles traditionnelles.

\textbf{Embeddings} = En apprentissage automatique, un \textit{embedding} désigne une technique d’apprentissage de représentations qui projette des données complexes et de haute dimension dans un espace vectoriel de dimension plus faible. Le terme désigne également la représentation obtenue, où des structures ou les relations significatives sont préservées. En tant que technique, il apprend ces vecteurs à partir de données telles que des mots, des images ou des interactions d’utilisateurs. Un espace vectoriel est un cadre mathématique dans lequel chaque entité est représentée par un vecteur, c’est-à-dire une suite ordonnée de valeurs numériques. Cette représentation permet d’exprimer des idées ou objets complexes sous la forme de points dans un espace à \textit{n} dimensions, rendant ainsi mesurables leurs similarités ou leurs différences.

\textbf{Espace vectoriel} = En \textit{machine learning}, la caractérisation d'une image par exemple sur base sur des embeddings. Ils sont des représentations vectorielles continues qui encapsulent le contexte et le sens des données. Ils traduisent des concepts complexes en un format numérique compact, capturant les relations sémantiques entre différentes entités. Cela se traduit par une représentation dense, de faible dimensionnalité, qui conserve des propriétés importantes des données d'origine.

\textbf{Fine-tuning} = Contrairement à l’entraînement initial (comme ce qui a été fait avec Clip par exemple) qui nécessite des jeux de données massifs, cet affinage se concentre sur des données plus restreintes et spécialisées. Il s’agit d’un processus itératif qui vise à améliorer la performance du modèle sur une tâche particulière, sans perdre les connaissances préalables acquises lors de l’entraînement initial.

\textbf{GPU} = \textit{Graphics Processing Unit}, c'est une unité de calcul assurant les fonctions de calcul d’image. Originellement les GPUs ont été développés pour les jeux vidéos et le calcul de déplacements en 2 ou 3 dimensions.

\textbf{Graphe de connaissance} = \textit{Knowledge graph}, est une base de connaissance modélisant ses données sous forme graphique. Beaucoup utilisé dans l'optimisation de moteurs de recherche, le plus connu est celui développé par Google qui fournit une information structurée et détaillée centrée sur l'objet de la recherche, en plus de la liste d'hyperliens vers d'autres sites. L'objectif est de permettre aux utilisateurs de résoudre leur requête sans avoir besoin de naviguer vers d'autres sites pour accéder aux informations capitales.

\textbf{HTR} = \textit{Handwritten Text Recognition}, c'est la reconnaissance automatique de caractères manuscrits sur une image.

\textbf{IDE} = \textit{Integrated development environment}, est une application logicielle qui aide les programmeurs à développer efficacement le code logiciel. Il augmente la productivité des développeurs en combinant les capacités telles que l'édition de logiciels, la construction, les tests et l'empaquetage dans une application facile à utiliser. De la même manière que les rédacteurs utilisent des éditeurs de texte et les comptables utilisent des feuilles de calcul, les développeurs de logiciels utilisent des IDE pour faciliter leur travail.

\textbf{IIIF} = \textit{International Image Interoperability Framework}, désigne à la fois une communauté et un ensemble de spécifications techniques dont l’objectif est de définir un cadre d’interopérabilité pour la diffusion et l'échange d’images haute résolution sur le Web. Le cadre normatif de IIIF définit des interfaces de programmation applicative (API) communes qui fournissent une méthode standardisée de description et d'accès aux images sur le Web, ainsi que l'exposition de métadonnées descriptives et structurelles nécessaires à la présentation d'un document numérique dans une interface.

\textbf{Layout recognition} = \textit{Document layout analysis}, est, en computer vision ou en traitement automatique du langage, le procédé par lequel on identifie ou catégorise des régions d'intérêts dans un document scanné ou une photo. On segmente les zones de textes, on détecte et annote les différentes zones d'une image. Ce procédé peut se faire par l'annotation humaine.

\textbf{LLM} = \textit{Large Language Models}, ou grand modèle de langue, est un modèle IA possédant un grand nombre de paramètres et capable de communiquer en langage naturel. Il est entraîné sur des grandes quantités de texte.
Un modèle de langue est un modèle probabiliste de la distribution d’éléments linguistiques (lettres, phonèmes, mots) dans une langue naturelle. Les plus connus sont des modèles génératifs qui calculent le mot suivant ou la lettre suivante dans une séquence de mots, selon un contexte, pour interagir avec l’utilisateur. Ce sont notamment les agents conversationnels utilisant l'intelligence artificielle comme ChatGPT, Mistral, Deepseek, etc,.

\textbf{Méthodes Agiles} = c'est une méthodologie de gestion de projet ouverte au changement, dont l’objectif est de développer un produit de haute qualité de façon incrémentale. Elle s’oppose aux méthodologies de gestion de projet traditionnelles qui s’organisent selon un mode de travail séquentiel. La méthode agile est organisée en cycles de développements courts, appelés des sprints. Le produit final est développé au fur et à mesure de l’avancement des sprints. L’équipe agile est invitée à collecter du \textit{feedback} le plus tôt possible auprès des utilisateurs du produit, afin de prendre en compte leurs remarques dans le prochain cycle de développement (le prochain sprint). Le produit est ainsi construit de façon collaborative. L’agilité, ou la méthode agile, apporte de la souplesse dans la gestion d’un projet, et offre la possibilité de changer le périmètre de projet en cours de route.

\textbf{Métriques} = \textit{Metrics}, est une valeur numérique qui permet de quantifier la qualité des prédictions d’un modèle. Son rôle est essentiel durant toutes les étapes du développement d’un modèle de Machine Learning car elle permet de déterminer si un modèle correspond à nos attentes. En fonction des résultats obtenus, les métriques permettent de comparer objectivement plusieurs modèles entre eux, de choisir le modèle le plus performant ou de changer les hyperparamètres d’un modèle.

\textbf{MLOps} = \textit{Machine Learning Model Operationalization Management}, est un ensemble de pratiques qui vise à déployer et maintenir des modèles d'apprentissage automatique en production de manière fiable et efficace.

\textbf{Modèle IA} = Un modèle d’IA est un algorithme, dont le fonctionnement est déterminé par un ensemble d’attributs, et qui est conçu pour opérer, selon les cas, différentes tâches, telles que la prédiction, la classification, l’inférence ou la génération. Par exemple, les modèles de réseaux de neurones profond, etc,.

\textbf{OCR} = \textit{Optical Character Recognition}, c'est la reconnaissance automatique de caractères imprimés sur une image. 

\textbf{Open-source} = Un logiciel Open Source est un code conçu pour être accessible au public : n'importe qui peut voir, modifier et distribuer le code à sa convenance. Ce type de logiciel est développé de manière collaborative et décentralisée, par une communauté, et repose sur l'examen par les pairs. Un logiciel Open Source est souvent moins cher, plus flexible et profite d'une longévité supérieure par rapport à ses équivalents propriétaires, car il est développé par des communautés et non par une entreprise ou un auteur.

\textbf{Orchestration des workflows} = \textit{Workflow orchestrator}, consiste à coordonner les tâches automatisées des applications et services métier afin de garantir une exécution fluide. Alors que l’automatisation des workflows consiste à automatiser les tâches individuellement, l’orchestration des workflows crée un écosystème au sein duquel ces tâches automatisées interagissent efficacement, suivent une séquence logique et s’intègrent à d’autres systèmes pour former un processus métier de bout en bout. En coordonnant les tâches (par exemple, le traitement de données, les notifications, les approbations et les mises à jour du système), l’orchestration des workflows permet de réduire le risque d’erreur et d’optimiser les opérations.

\textbf{Plug-in} = Le terme plugin provient de la métaphore de la prise électrique standardisée, et souligne le fait que le logiciel hôte est conçu pour accueillir des logiciels étendant ses fonctionnalités, par opposition aux ajouts non prévus initialement apportés à l'aide de correctifs (patchs).

\textbf{Poids et biais} = Dans l'apprentissage automatique, un modèle est entraîné pour faire des prédictions ou effectuer des tâches spécifiques basées sur des données d'entrée. Le modèle apprend les modèles et les relations au sein des données grâce à un processus appelé formation. Pendant l'entraînement, le modèle ajuste ses paramètres internes, qui incluent les pondérations et les biais, pour minimiser la différence entre ses prédictions et les valeurs réelles des données d'entraînement.

\textbf{Port} = Un port réseau est un port virtuel point final dans un réseau informatique, utilisé pour la communication entre appareils. Chaque port est identifié par un numéro, facilitant le transfert de données et les services réseau comme HTTP ou FTP.

\textbf{Prompt} = c’est tout simplement une instruction destinée à une intelligence artificielle générative. C’est-à-dire, les IA capables de générer du contenu, qu’il s’agisse d’un texte, d’une image ou même d’une musique. Grâce à sa compréhension du langage naturel, l’algorithme d’IA va ensuite analyser l’ensemble de vos instructions pour y répondre.

\textbf{Python} = Le langage Python est un langage de programmation open source multi-plateformes et orienté objet. Grâce à des bibliothèques spécialisées, Python s'utilise pour de nombreuses situations comme le développement logiciel, le \textit{machine learning}, l'analyse de données, ou la gestion d'infrastructures. 

\textbf{Reconnaissance d'Entités Nommées} = \textit{Name Entity Recognition}, (NER) est une tâche secondaire de l'extraction d'informations dans le traitement du langage naturel (NLP) qui classe les entités nommées dans des catégories prédéfinies telles que les noms de personnes, les organisations, les lieux, les codes médicaux, les expressions temporelles, les quantités, les valeurs monétaires, et bien d'autres encore.

\textbf{Réseau de neurones} = \textit{Neural network}, ils sont généralement optimisés par des méthodes d'apprentissage de type probabiliste, se compose de plusieurs couches, une couche d'entrée qui reçoit les données, plusieurs couches cachées qui transforment les données pour en extraire des caractéristiques et une couche de sortie qui produit la prédiction finale. Ils servent à détecter des catégories d'images ou des coordonnées d'objets et sont souvent utilisés en \textit{machine learning} et spécifiquement en \textit{deep learning}.

\textbf{Réseau de neurones convolutifs} = \textit{Convolutionnal neural network}, sont comme les réseaux de neurones sauf qu'ils considèrent explicitement que l'input est une image. Il est inspiré du cortex visuel des animaux. Les neurones de cette région du cerveau sont arrangés de sorte qu'ils correspondent à des régions qui se chevauchent lors du pavage du champ visuel. Leur fonctionnement est inspiré par les processus biologiques, ils consistent en un empilage multicouche de perceptrons, dont le but est de prétraiter de petites quantités d'informations. Les réseaux neuronaux convolutifs ont de larges applications dans la reconnaissance d'image et vidéo, les systèmes de recommandation et le traitement du langage naturel.

\textbf{SaaS} = \textit{Software as a Service} = ou logiciel en tant que service est un modèle d'exploitation commerciale des logiciels dans lequel ceux-ci sont installés sur des serveurs distants plutôt que sur la machine de l'utilisateur. Les clients ne paient pas de licence d'utilisation pour une version, mais utilisent librement le service en ligne ou, plus généralement, payent un abonnement.

\textbf{Scikit-learn} = est une bibliothèque libre Python destinée à l'apprentissage automatique. Elle propose dans son framework de nombreuses bibliothèques d’algorithmes à implémenter, clé en main. Ces bibliothèques sont à disposition notamment des data scientists. Elle comprend notamment des fonctions pour estimer des forêts aléatoires, des régressions logistiques, des algorithmes de classification, et les machines à vecteurs de support. Elle est conçue pour s'harmoniser avec d'autres bibliothèques libres Python, notamment NumPy et SciPy.

\textbf{Scraping} = Le scraping définit de façon générale une technique permettant d'extraire du contenu (des informations) d'un ou de plusieurs sites web de manière totalement automatique. Ce sont des scripts, des programmes informatiques, qui sont chargés d'extraire ces informations.

\textbf{Script} = En informatique, un script désigne un programme (ou un bout de programme) chargé d'exécuter une action pré-définie quand un utilisateur réalise une action ou qu'une page web est en cours d'affichage sur un écran. Il s'agit d'une suite de commandes simples et souvent peu structurées qui permettent l'automatisation de certaines tâches successives dans un ordre donné.

\textbf{Silence} = Le silence est l'opposé du bruit, là ou dans le bruit le modèle trouve des choses qu'il ne devrait pas trouver ou rapprocher de ce qu'on lui a demandé, le silence lui marque l'absence de résultat du modèle alors même qu'il devrait y avoir des résultats.

\textbf{TensorFlow} = est un outil open source d'apprentissage automatique développé par Google. Le code source a été ouvert le 9 novembre 2015 par Google et publié sous licence Apache.

\textbf{Transformers} = En apprentissage profond, un \textit{transformer} est une architecture basée sur le mécanisme d’attention multi-têtes, dans laquelle le texte est converti en représentations numériques appelées \textit{tokens}. Chaque \textit{token} est ensuite transformé en un vecteur grâce à une recherche (\textit{lookup}) dans une table d’\textit{embeddings} de mots. À chaque couche, chaque \textit{token} est ensuite contextualisé dans la limite de la fenêtre de contexte avec les autres \textit{tokens} (non masqués) via un mécanisme d’attention multi-têtes exécuté en parallèle. Ce processus permet d’amplifier le signal des \textit{tokens} importants et d’atténuer celui des \textit{tokens} moins pertinents.

\textbf{User-friendly} = est un terme qui décrit la facilité d’utilisation d’un site web ou d’un logiciel, c’est-à-dire que les utilisateurs peuvent utiliser le système technique de manière simple et intuitive. Plus l’utilisateur comprend facilement et rapidement l’application, plus elle est \enquote{\textit{User-friendly}}.

\textbf{Vérité terrain} = \textit{Ground Truth}, fait référence aux informations réelles et vérifiées qui servent de référence pour évaluer l'exactitude des modèles et des algorithmes basés sur les données. Ce sont les données annotées à la main par les personnes qui entraînent les modèles, et sur lesquels les modèles se basent pour leur prédictions. C'est notamment en comparant la vérité terrain et les données générées que nous pouvons mesurer les performances du modèle.

\textbf{Vision-language model} = les VLM sont des modèles d’intelligence artificielle qui associent des capacités de vision par ordinateur et de traitement automatique du langage naturel (NLP). Les VLM apprennent à mapper les relations entre les données textuelles et les données visuelles telles que les images ou les vidéos, ce qui permet à ces modèles de générer du texte à partir d’entrées visuelles ou de comprendre des prompts en langage naturel dans le contexte d’informations visuelles.

\textbf{Vision par ordinateur} = \textit{computer vision}, est une branche de l'intelligence artificielle dont le principal but est de permettre à une machine d'analyser et traiter une ou plusieurs images ou vidéos prises par un système d'acquisition.